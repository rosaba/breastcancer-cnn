{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConV Network: Setup, training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # version 1.3.1\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CyclicLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# data augmentation\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "from PIL import ImageFilter\n",
    "\n",
    "# Split arrays or matrices into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# remove if not needed because augmentation is already applied \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# INSTALL tqdm for jupyter lab:\n",
    "# 1. pip install tqdm==4.36.1\n",
    "# 2. pip install ipywidgets\n",
    "# 3. jupyter nbextension enable --py widgetsnbextension\n",
    "# 4. jupyter labextension install @jupyter-widgets/jupyterlab-manager (installed nodejs and npm needed)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#visualization at the end:\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary data from file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_train_df = pd.read_json(\"dataframes/final_train_df.json\")\n",
    "loaded_val_df = pd.read_json(\"dataframes/val_df.json\")\n",
    "loaded_test_df = pd.read_json(\"dataframes/test_df.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>801</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>801</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>851</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>601</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280827</th>\n",
       "      <td>9173</td>\n",
       "      <td>data/train_class1_augmented/9173_idx5_x2301_y1...</td>\n",
       "      <td>1</td>\n",
       "      <td>2301</td>\n",
       "      <td>1601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280828</th>\n",
       "      <td>13693</td>\n",
       "      <td>data/train_class1_augmented/13693_idx5_x551_y1...</td>\n",
       "      <td>1</td>\n",
       "      <td>551</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280829</th>\n",
       "      <td>13402</td>\n",
       "      <td>data/train_class1_augmented/13402_idx5_x1451_y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1451</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280830</th>\n",
       "      <td>16165</td>\n",
       "      <td>data/train_class1_augmented/16165_idx5_x1401_y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1401</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280831</th>\n",
       "      <td>14157</td>\n",
       "      <td>data/train_class1_augmented/14157_idx5_x1451_y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1451</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280832 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        patient_id                                         image_path  label  \\\n",
       "0            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "1            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "2            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "3            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "4            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "...            ...                                                ...    ...   \n",
       "280827        9173  data/train_class1_augmented/9173_idx5_x2301_y1...      1   \n",
       "280828       13693  data/train_class1_augmented/13693_idx5_x551_y1...      1   \n",
       "280829       13402  data/train_class1_augmented/13402_idx5_x1451_y...      1   \n",
       "280830       16165  data/train_class1_augmented/16165_idx5_x1401_y...      1   \n",
       "280831       14157  data/train_class1_augmented/14157_idx5_x1451_y...      1   \n",
       "\n",
       "           x     y  \n",
       "0        801  1151  \n",
       "1        801   951  \n",
       "2        851   651  \n",
       "3        601   951  \n",
       "4       1001   851  \n",
       "...      ...   ...  \n",
       "280827  2301  1601  \n",
       "280828   551  1551  \n",
       "280829  1451  1001  \n",
       "280830  1401  1501  \n",
       "280831  1451  1251  \n",
       "\n",
       "[280832 rows x 5 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show example file:\n",
    "loaded_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10430</th>\n",
       "      <td>9178</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1451</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10431</th>\n",
       "      <td>9178</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>751</td>\n",
       "      <td>1051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432</th>\n",
       "      <td>9178</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>801</td>\n",
       "      <td>1301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10433</th>\n",
       "      <td>9178</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1551</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10434</th>\n",
       "      <td>9178</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1801</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17196</th>\n",
       "      <td>10259</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1301</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17197</th>\n",
       "      <td>10259</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17198</th>\n",
       "      <td>10259</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>2651</td>\n",
       "      <td>1601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17199</th>\n",
       "      <td>10259</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1401</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17200</th>\n",
       "      <td>10259</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>2501</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       patient_id                                         image_path  label  \\\n",
       "10430        9178  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "10431        9178  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "10432        9178  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "10433        9178  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "10434        9178  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "...           ...                                                ...    ...   \n",
       "17196       10259  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "17197       10259  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "17198       10259  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "17199       10259  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "17200       10259  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "\n",
       "          x     y  \n",
       "10430  1451  1551  \n",
       "10431   751  1051  \n",
       "10432   801  1301  \n",
       "10433  1551   451  \n",
       "10434  1801   501  \n",
       "...     ...   ...  \n",
       "17196  1301   401  \n",
       "17197   251   901  \n",
       "17198  2651  1601  \n",
       "17199  1401   501  \n",
       "17200  2501   551  \n",
       "\n",
       "[3000 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for testing reasons:\n",
    "sample_train = loaded_train_df.head(20000)\n",
    "sample_val = loaded_val_df.head(3000)\n",
    "sample_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting hyperparameters for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #bisher 32; beim model von letzter nacht 128\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.001 #beim model von letzter nacht 0.002\n",
    "NUM_EPOCHS = 6 #vorher 8; beim letzten mal 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directory for saving the model after training (only execute once):\n",
    "#os.mkdir(\"saved_models\")\n",
    "\n",
    "OUTPUT_PATH = \"saved_models/model_version_\"\n",
    "MODEL_EVAL_PATH = \"saved_models/model_eval_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data\n",
    "\n",
    "This allows us to manipulate our three datasets in a specified way.\n",
    "\n",
    "In our first notebook on data preprocessing we already augmented the image patches in the training set with class 1 to boost their size. The goal was to reach an equal size for both classes 0 and 1. Here we still add another transformation on the training data (horizontal and vertical flipping plus rotation) to boost the overall size of our training data (now both classes).\n",
    "\n",
    "Further we need to convert the input data to PyTorch tensor.\n",
    "\n",
    "Additionally we add normalization. Neural networks train better when the input data is normalized, so that the data ranges from -1 to 1 or from 0 to 1. For that we need to compute the standard deviation and the mean of our training data and apply them to our train_transform. \n",
    "Note: for each input channel (here: 3) we need to apply the according mean and standard deviation.\n",
    "\n",
    "The last two transformations count also for the validation and test datasets, whereas the first mentioned transformations only account for our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean and standard deviation of our three datasets\n",
    "\n",
    "To hand the right parameters for mean and standard deviation to our my_transform function further down we first create three simple dataloader objects with our corresponding datasets to calculate the mean and standard deviation respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple transform method; only for purpose of calculation mean and standard deviation\n",
    "def transform_to_tensor():\n",
    "    train_trans = [transforms.Resize((50, 50)), transforms.ToTensor()]\n",
    "    return transforms.Compose(train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test train_loader to calculate mean and standard dev on:\n",
    "train_set = BreastCancerDataset(loaded_train_df, transform=transform_to_tensor())\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "\n",
    "for _,data in enumerate(train_loader):\n",
    "    images = data[\"image\"].to(device)\n",
    "    batch_samples = images.size(0) \n",
    "    data = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    nb_samples += batch_samples\n",
    "    \n",
    "mean /= nb_samples\n",
    "std /= nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are the mean values over our three dimensions in the train dataset:\n",
      "tensor([0.7718, 0.6068, 0.7322])\n",
      "\n",
      "Those are the standard deviation values over our three dims in train dataset:\n",
      "tensor([0.0926, 0.1417, 0.1081])\n"
     ]
    }
   ],
   "source": [
    "print(\"Those are the mean values over our three dimensions in the train dataset:\")\n",
    "print(str(mean))\n",
    "print()\n",
    "print(\"Those are the standard deviation values over our three dims in train dataset:\")\n",
    "print(str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied in values calculated above so we don't have to execute the two cells more often:\n",
    "means_train = [0.7718, 0.6068, 0.7322]\n",
    "standard_dev_train = [0.0926, 0.1417, 0.1081]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean and standard deviation of our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test_loader to calculate mean and standard dev for the test dataset:\n",
    "test_set = BreastCancerDataset(loaded_test_df, transform=transform_to_tensor())\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = 0.\n",
    "test_std = 0.\n",
    "test_nb_samples = 0.\n",
    "\n",
    "for _,data in enumerate(test_loader):\n",
    "    images = data[\"image\"].to(device)\n",
    "    t_batch_samples = images.size(0) \n",
    "    data = images.view(t_batch_samples, images.size(1), -1)\n",
    "    test_mean += data.mean(2).sum(0)\n",
    "    test_std += data.std(2).sum(0)\n",
    "    test_nb_samples += t_batch_samples\n",
    "    \n",
    "test_mean /= test_nb_samples\n",
    "test_std /= test_nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are the mean values over our three dimensions in the test dataset:\n",
      "tensor([0.8208, 0.6423, 0.7406])\n",
      "\n",
      "Those are the standard deviation values over our three dims in test dataset:\n",
      "tensor([0.0892, 0.1410, 0.1037])\n"
     ]
    }
   ],
   "source": [
    "print(\"Those are the mean values over our three dimensions in the test dataset:\")\n",
    "print(str(test_mean))\n",
    "print()\n",
    "print(\"Those are the standard deviation values over our three dims in test dataset:\")\n",
    "print(str(test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied in values:\n",
    "#test_mean = [0.8208, 0.6423, 0.7406]\n",
    "#test_std = [0.0892, 0.1410, 0.1037]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean and standard deviation of our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create val_loader to calculate mean and standard dev for the validation dataset:\n",
    "val_set = BreastCancerDataset(loaded_val_df, transform=transform_to_tensor())\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mean = 0.\n",
    "val_std = 0.\n",
    "val_nb_samples = 0.\n",
    "\n",
    "for _,data in enumerate(val_loader):\n",
    "    images = data[\"image\"].to(device)\n",
    "    v_batch_samples = images.size(0) \n",
    "    data = images.view(v_batch_samples, images.size(1), -1)\n",
    "    val_mean += data.mean(2).sum(0)\n",
    "    val_std += data.std(2).sum(0)\n",
    "    val_nb_samples += v_batch_samples\n",
    "    \n",
    "val_mean /= val_nb_samples\n",
    "val_std /= val_nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are the mean values over our three dimensions in the validation dataset:\n",
      "tensor([0.8031, 0.6202, 0.7270])\n",
      "\n",
      "Those are the standard deviation values over our three dims in validation dataset:\n",
      "tensor([0.0938, 0.1420, 0.1049])\n"
     ]
    }
   ],
   "source": [
    "print(\"Those are the mean values over our three dimensions in the validation dataset:\")\n",
    "print(str(val_mean))\n",
    "print()\n",
    "print(\"Those are the standard deviation values over our three dims in validation dataset:\")\n",
    "print(str(val_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied in values:\n",
    "#val_mean = [0.8031, 0.6202, 0.7270]\n",
    "#val_std = [0.0938, 0.1420, 0.1049]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following transform method will be used to manipulate our data sets for training and evaluation\n",
    "\n",
    "Here we add the calculated values for mean and standard deviation for purpose of normalization from above.\n",
    "For now we take the same values also for the validation and test data set, as we assume them to be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms to apply to our data:\n",
    "def my_transform(key=\"train_transform\"):\n",
    "    #boost class 1 in training set:\n",
    "    train_transform = [transforms.Resize((50, 50)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.RandomRotation(90), \n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.7718, 0.6068, 0.7322],std=[0.0926, 0.1417, 0.1081])]\n",
    "    #change values:\n",
    "    val_transform = [transforms.Resize((50, 50)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.8031, 0.6202, 0.7270],std=[0.0938, 0.1420, 0.1049])]\n",
    "    \n",
    "    test_transform = [transforms.Resize((50, 50)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.8208, 0.6423, 0.7406],std=[0.0892, 0.1410, 0.1037])]\n",
    "        \n",
    "    data_transforms = {'train_transform': transforms.Compose(train_transform),\n",
    "                       'val_transform': transforms.Compose(val_transform),\n",
    "                       'test_transform': transforms.Compose(test_transform)}\n",
    "    return data_transforms[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we create a class for our various datasets\n",
    "\n",
    "This class will be used to create our various datasets and subsequently be passed to our data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, transform=None):\n",
    "        self.states = df\n",
    "        self.transform=transform\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.states.patient_id.values[idx]\n",
    "        x_coord = self.states.x.values[idx]\n",
    "        y_coord = self.states.y.values[idx]\n",
    "        image_path = self.states.image_path.values[idx] \n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB') # try to convert to YUV instead of RGB later\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "         \n",
    "        label = np.int(self.states.label.values[idx])\n",
    "        return {\"image\": image,\n",
    "                \"label\": label,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"x\": x_coord,\n",
    "                \"y\": y_coord}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting our data sets (training, validation, testing) into a dataloader object\n",
    "\n",
    "Here we create our datasets and apply the transformations defined above in our my_transform method.\n",
    "After we befill our dataloader with the datasets.\n",
    "\n",
    "The data loader object in PyTorch provides a number of features which are useful for us in consuming training data.\n",
    "We can automatically shuffle the data. Further we can easily batch the data according to our defined BATCH_SIZE. PyTorch also enables us to load the data in parallel using multiprocessing. Data consumption becomes a lot more efficient this way.\n",
    "\n",
    "As we say later on in the training loop the PyTorch data loader object can be used as an iterator, i.e. use \"enumerate\" to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BreastCancerDataset(loaded_train_df, transform=my_transform(key=\"train_transform\"))\n",
    "val_dataset = BreastCancerDataset(loaded_val_df, transform=my_transform(key=\"val_transform\"))\n",
    "test_dataset = BreastCancerDataset(loaded_test_df, transform=my_transform(key=\"test_transform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\", \"test\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 280832, 'val': 37886, 'test': 43313}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the size of our three different data sets:\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader, \"test\": test_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training dataset has the size: 280832\n",
      "Divided by our batch_size of 64 this leads to 4388 iterations for each epoch while training.\n",
      "\n",
      "Our validation data set has the size: 37886\n",
      "Divided by our batch_size of 64 this leads to 591 iterations for each epoch for validation.\n",
      "\n",
      "Our test data set has the size: 43313\n",
      "Divided by our batch_size of 64 this leads to 677 iterations for each epoch for testing.\n"
     ]
    }
   ],
   "source": [
    "#meaning of number stored in dataloader: nr iterations for each epoch\n",
    "print(\"Our training dataset has the size: \" + str(dataset_sizes[\"train\"]))\n",
    "print(\"Divided by our batch_size of \" + str(BATCH_SIZE) + \" this leads to \" + \n",
    "      str(len(dataloaders[\"train\"])) + \" iterations for each epoch while training.\")\n",
    "print()\n",
    "print(\"Our validation data set has the size: \" + str(dataset_sizes[\"val\"]))\n",
    "print(\"Divided by our batch_size of \" + str(BATCH_SIZE) + \" this leads to \" + \n",
    "      str(len(dataloaders[\"val\"])) + \" iterations for each epoch for validation.\")\n",
    "print()\n",
    "print(\"Our test data set has the size: \" + str(dataset_sizes[\"test\"]))\n",
    "print(\"Divided by our batch_size of \" + str(BATCH_SIZE) + \" this leads to \" + \n",
    "      str(len(dataloaders[\"test\"])) + \" iterations for each epoch for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the architecture of our ConV neural network\n",
    "\n",
    "Here we define the architecture of our model. For that we create our own class which inherits from the nn.Module super class from PyTorch.\n",
    "For all functions that can be used from that super class see here: https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "With the nn.Sequential function we define the various layers in our network.\n",
    "Our network adapts a 3-layer CNN architecture employing 16, 32, 36.992 and 128 neurons for our first two convolutional layers and the two fully-connected layers respectively. We chose these numbers as we planned to orient our approach at the architecture chosen in this paper: 2014 ...\n",
    "Note: We had to add an additional fully-connected layer with 36.992 neurons. (Name reason TODO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the paper they chose a fixed convolutional kernel of size 8x8 and a pool kernel of size 2x2.\n",
    "\n",
    "\n",
    "We used ReLu as activation functions and maxPooling at the end of our convolutional layers to down-sample our data after each convolution by reducing the effective image size.\n",
    "Further we applied dropout to minimize the chance of overfitting, improve generalization and also improve the training time. In order to let the dropout layer do its job we have to flatten the input to that layer in the forward function. For further reasoning see here: https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2\n",
    "\n",
    "\n",
    "### TODO's:\n",
    "- Grund warum 2. fully connected layer nötig? haben wir gerade ein 4-layer oder ein 3-layer netz?\n",
    "    (wegen anwendung von dropout und dessen reshaping/flattening nötig? irgendwie so war es ja..)\n",
    "- vielleicht genauer begründen warum wir ReLu nehmen, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General method to calculate either the output size, the convolutional filter (kernel) size, the stride, or pooling:\n",
    "\n",
    "<math display=\"block\">\n",
    "  <msub>\n",
    "    <mi>W</mi>\n",
    "    <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "      <mi>o</mi>\n",
    "      <mi>u</mi>\n",
    "      <mi>t</mi>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <mo>=</mo>\n",
    "  <mfrac>\n",
    "    <mrow>\n",
    "      <mo stretchy=\"false\">(</mo>\n",
    "      <msub>\n",
    "        <mi>W</mi>\n",
    "        <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "          <mi>i</mi>\n",
    "          <mi>n</mi>\n",
    "        </mrow>\n",
    "      </msub>\n",
    "      <mo>&#x2013;</mo>\n",
    "      <mi>F</mi>\n",
    "      <mo>+</mo>\n",
    "      <mn>2</mn>\n",
    "      <mi>P</mi>\n",
    "      <mo stretchy=\"false\">)</mo>\n",
    "    </mrow>\n",
    "      <mi>/</mi>  \n",
    "    <mi>S</mi>\n",
    "  </mfrac>\n",
    "  <mo>+</mo>\n",
    "  <mn>1</mn>\n",
    "</math>\n",
    "\n",
    "- W in = width of the input\n",
    "- F = filter or kernel size\n",
    "- P = padding\n",
    "- S = stride\n",
    "- W out = output size\n",
    "\n",
    "The same formula accounts for the calculation of the height of the input and output, as we have the case of a symmetrical image size (height = width) and we also decided to take an equal number for height and width of the filter/kernel.\n",
    "\n",
    "### TODO \n",
    "- maybe show calculation why we chose which number for kernel size/stride etc or just argument with numbers from paper as done so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # ancestor constructor call\n",
    "        super(ThreeLayerCNN, self).__init__()\n",
    "        \n",
    "        # Conv Layer 1\n",
    "        self.convLayer1 = nn.Sequential( # TODO: calculate stride, padding\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=8, stride=1),\n",
    "            nn.BatchNorm2d(16), #NEU\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "        \n",
    "        # Conv Layer 2\n",
    "        self.convLayer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=8, stride=1),\n",
    "            nn.BatchNorm2d(32), #NEU\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "            \n",
    "        # Dropout: avoid overfitting. only for training\n",
    "        self.drop_out = nn.Dropout()        \n",
    "        \n",
    "        #Fully Connected Layer\n",
    "      #  self.fc1 = nn.Linear(128, 2) #      32, 34, 34 \n",
    "        self.fc1 = nn.Linear(32*34*34, 128)\n",
    "       # self.fc1 = nn.Linear(32*1*1, 128) # 32,1,1\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.convLayer1(x)\n",
    "        #print(\"after conv1: \" + str(out.shape))\n",
    "        out = self.convLayer2(out)\n",
    "        #print(\"after conv2: \" + str(out.shape)) # after conv2: torch.Size([32, 32, 34, 34])\n",
    "        # what does reshape do? -> flatten. why befor drop_out?\n",
    "       # out = out.reshape(out.size(0), -1) \n",
    "       # out = out.view(-1, 128)\n",
    "        out = out.view(-1, 32*34*34)\n",
    "       # out = out.view(-1, 32*1*1)\n",
    "        out = self.drop_out(out)\n",
    "        #print(\"after dropout: \" + str(out.shape))\n",
    "        out = self.fc1(out)\n",
    "        #print(\"after fc1: \" + str(out.shape))\n",
    "        out = self.fc2(out)\n",
    "        #print(\"after fc2: \" + str(out.shape))\n",
    "        \n",
    "        return out      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThreeLayerCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeLayerCNN(\n",
      "  (convLayer1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(8, 8), stride=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convLayer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(8, 8), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (drop_out): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=36992, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a loss function and an optimizer\n",
    "\n",
    "Before we train the model, we have to define our loss function and optimizer.\n",
    "We used PyTorch’s CrossEntropyLoss() function.\n",
    "\n",
    "Notice: So far we haven't defined a softmax activation for the final fully-connected classification layer above.\n",
    "The reason is that the crossentropy function from PyTorch combines both a softmax activation together with a cross entropy loss function.\n",
    "\n",
    "After that we define an adam optimizer, which we pass our parameters that we want the optimizer to train. We also supply our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every epoch\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/312], Loss: 0.3119, Accuracy: 84.38%\n",
      "Epoch [1/2], Step [200/312], Loss: 0.3614, Accuracy: 82.81%\n",
      "Epoch [1/2], Step [300/312], Loss: 0.3453, Accuracy: 85.94%\n",
      "Epoch [2/2], Step [100/312], Loss: 0.3614, Accuracy: 87.50%\n",
      "Epoch [2/2], Step [200/312], Loss: 0.3334, Accuracy: 84.38%\n",
      "Epoch [2/2], Step [300/312], Loss: 0.2366, Accuracy: 90.62%\n"
     ]
    }
   ],
   "source": [
    "#number of iterations:\n",
    "total_step = len(train_dataloader) \n",
    "\n",
    "#for tracking the loss and accuracy of our model:\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  \n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"label\"].to(device)        \n",
    "        \n",
    "        # run the forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        # run the backward pass (backpropagation) and perform adams optimization as specified above\n",
    "        optimizer.zero_grad() # here the gradients are zeroed before backprop starts\n",
    "        loss.backward() # calcs the gradients\n",
    "        optimizer.step() #adam optimizer training step\n",
    "        \n",
    "        #track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1) #richtig??\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        \n",
    "        #print out current results after another 100 iterations are done:\n",
    "        if (i+1) % 100 == 0: # TODO: change back to 100\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%' \n",
    "                   .format(epoch+1, NUM_EPOCHS, i+1, total_step, loss.item(), (correct / total) * 100))\n",
    "\n",
    "            \n",
    "#Save our model to new directory\n",
    "#to keep track of different models created we append a timestamp to the model name\n",
    "time = datetime.now(tz=None)\n",
    "timestamp = str(time.year) + \"_\" + str(time.month) + \"_\" + str(time.day) + \"_\" + str(time.hour)+ \":\" + str(time.minute)+ \":\" + str(time.second)\n",
    "\n",
    "new_model_new_dir = OUTPUT_PATH + timestamp\n",
    "os.mkdir(new_model_new_dir)\n",
    "curr_model_output_path = new_model_new_dir + \"/model_version_\" + timestamp + \".pth\"\n",
    "torch.save(model.state_dict(), curr_model_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbindung train und val:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase train, Epoch [1/2], Step [10/312], Loss: 0.2389, Accuracy: 92.19%\n",
      "Phase train, Epoch [1/2], Step [20/312], Loss: 0.3383, Accuracy: 89.06%\n",
      "Phase train, Epoch [1/2], Step [30/312], Loss: 0.3550, Accuracy: 87.50%\n",
      "Phase train, Epoch [1/2], Step [40/312], Loss: 0.4550, Accuracy: 85.94%\n",
      "Phase train, Epoch [1/2], Step [50/312], Loss: 0.3184, Accuracy: 90.62%\n",
      "Phase train, Epoch [1/2], Step [60/312], Loss: 0.2823, Accuracy: 87.50%\n",
      "Phase train, Epoch [1/2], Step [70/312], Loss: 0.3356, Accuracy: 84.38%\n",
      "Phase train, Epoch [1/2], Step [80/312], Loss: 0.2563, Accuracy: 92.19%\n",
      "Phase train, Epoch [1/2], Step [90/312], Loss: 0.3465, Accuracy: 89.06%\n",
      "Phase train, Epoch [1/2], Step [100/312], Loss: 0.3539, Accuracy: 82.81%\n",
      "Phase train, Epoch [1/2], Step [110/312], Loss: 0.4632, Accuracy: 75.00%\n",
      "Phase train, Epoch [1/2], Step [120/312], Loss: 0.3961, Accuracy: 82.81%\n",
      "Phase train, Epoch [1/2], Step [130/312], Loss: 0.2818, Accuracy: 87.50%\n",
      "Phase train, Epoch [1/2], Step [140/312], Loss: 0.5584, Accuracy: 76.56%\n",
      "Phase train, Epoch [1/2], Step [150/312], Loss: 0.3156, Accuracy: 87.50%\n",
      "Phase train, Epoch [1/2], Step [160/312], Loss: 0.3477, Accuracy: 82.81%\n",
      "Phase train, Epoch [1/2], Step [170/312], Loss: 0.3181, Accuracy: 85.94%\n",
      "Phase train, Epoch [1/2], Step [180/312], Loss: 0.3304, Accuracy: 92.19%\n",
      "Phase train, Epoch [1/2], Step [190/312], Loss: 0.3125, Accuracy: 87.50%\n",
      "Phase train, Epoch [1/2], Step [200/312], Loss: 0.3955, Accuracy: 84.38%\n",
      "Phase train, Epoch [1/2], Step [210/312], Loss: 0.3002, Accuracy: 89.06%\n",
      "Phase train, Epoch [1/2], Step [220/312], Loss: 0.3526, Accuracy: 85.94%\n",
      "Phase train, Epoch [1/2], Step [230/312], Loss: 0.3518, Accuracy: 85.94%\n",
      "Phase train, Epoch [1/2], Step [240/312], Loss: 0.3342, Accuracy: 85.94%\n",
      "Phase train, Epoch [1/2], Step [250/312], Loss: 0.4621, Accuracy: 79.69%\n",
      "Phase train, Epoch [1/2], Step [260/312], Loss: 0.3332, Accuracy: 82.81%\n",
      "Phase train, Epoch [1/2], Step [270/312], Loss: 0.7847, Accuracy: 71.88%\n",
      "Phase train, Epoch [1/2], Step [280/312], Loss: 0.3426, Accuracy: 84.38%\n",
      "Phase train, Epoch [1/2], Step [290/312], Loss: 0.3609, Accuracy: 79.69%\n",
      "Phase train, Epoch [1/2], Step [300/312], Loss: 0.3823, Accuracy: 82.81%\n",
      "Phase train, Epoch [1/2], Step [310/312], Loss: 0.2751, Accuracy: 90.62%\n",
      "train Epoch loss: 0.3739\n",
      "Phase val, Epoch [1/2], Step [10/312], Loss: 0.4278, Accuracy: 81.25%\n",
      "Phase val, Epoch [1/2], Step [20/312], Loss: 0.5411, Accuracy: 81.25%\n",
      "Phase val, Epoch [1/2], Step [30/312], Loss: 0.8710, Accuracy: 60.94%\n",
      "Phase val, Epoch [1/2], Step [40/312], Loss: 0.7051, Accuracy: 68.75%\n",
      "val Epoch loss: 0.5296\n",
      "Phase train, Epoch [2/2], Step [10/312], Loss: 0.3846, Accuracy: 84.38%\n",
      "Phase train, Epoch [2/2], Step [20/312], Loss: 0.3375, Accuracy: 89.06%\n",
      "Phase train, Epoch [2/2], Step [30/312], Loss: 0.2712, Accuracy: 89.06%\n",
      "Phase train, Epoch [2/2], Step [40/312], Loss: 0.3117, Accuracy: 90.62%\n",
      "Phase train, Epoch [2/2], Step [50/312], Loss: 0.2321, Accuracy: 89.06%\n",
      "Phase train, Epoch [2/2], Step [60/312], Loss: 0.3084, Accuracy: 95.31%\n",
      "Phase train, Epoch [2/2], Step [70/312], Loss: 0.4388, Accuracy: 89.06%\n",
      "Phase train, Epoch [2/2], Step [80/312], Loss: 0.3578, Accuracy: 85.94%\n",
      "Phase train, Epoch [2/2], Step [90/312], Loss: 0.2905, Accuracy: 89.06%\n",
      "Phase train, Epoch [2/2], Step [100/312], Loss: 0.2238, Accuracy: 87.50%\n",
      "Phase train, Epoch [2/2], Step [110/312], Loss: 0.2574, Accuracy: 87.50%\n",
      "Phase train, Epoch [2/2], Step [120/312], Loss: 0.5236, Accuracy: 82.81%\n",
      "Phase train, Epoch [2/2], Step [130/312], Loss: 0.3568, Accuracy: 81.25%\n",
      "Phase train, Epoch [2/2], Step [140/312], Loss: 0.3336, Accuracy: 81.25%\n",
      "Phase train, Epoch [2/2], Step [150/312], Loss: 0.2204, Accuracy: 93.75%\n",
      "Phase train, Epoch [2/2], Step [160/312], Loss: 0.2785, Accuracy: 87.50%\n",
      "Phase train, Epoch [2/2], Step [170/312], Loss: 0.3111, Accuracy: 89.06%\n",
      "Phase train, Epoch [2/2], Step [180/312], Loss: 0.3503, Accuracy: 89.06%\n",
      "Phase train, Epoch [2/2], Step [190/312], Loss: 0.4599, Accuracy: 79.69%\n",
      "Phase train, Epoch [2/2], Step [200/312], Loss: 0.5774, Accuracy: 73.44%\n",
      "Phase train, Epoch [2/2], Step [210/312], Loss: 0.2760, Accuracy: 90.62%\n",
      "Phase train, Epoch [2/2], Step [220/312], Loss: 0.3785, Accuracy: 84.38%\n",
      "Phase train, Epoch [2/2], Step [230/312], Loss: 0.4285, Accuracy: 84.38%\n",
      "Phase train, Epoch [2/2], Step [240/312], Loss: 0.5164, Accuracy: 75.00%\n",
      "Phase train, Epoch [2/2], Step [250/312], Loss: 0.3981, Accuracy: 81.25%\n",
      "Phase train, Epoch [2/2], Step [260/312], Loss: 0.2559, Accuracy: 89.06%\n",
      "Phase train, Epoch [2/2], Step [270/312], Loss: 0.5115, Accuracy: 75.00%\n",
      "Phase train, Epoch [2/2], Step [280/312], Loss: 0.3965, Accuracy: 82.81%\n",
      "Phase train, Epoch [2/2], Step [290/312], Loss: 0.3278, Accuracy: 85.94%\n",
      "Phase train, Epoch [2/2], Step [300/312], Loss: 0.2883, Accuracy: 89.06%\n",
      "Phase train, Epoch [2/2], Step [310/312], Loss: 0.3587, Accuracy: 81.25%\n",
      "train Epoch loss: 0.3673\n",
      "Phase val, Epoch [2/2], Step [10/312], Loss: 0.3284, Accuracy: 90.62%\n",
      "Phase val, Epoch [2/2], Step [20/312], Loss: 0.4018, Accuracy: 85.94%\n",
      "Phase val, Epoch [2/2], Step [30/312], Loss: 0.5768, Accuracy: 71.88%\n",
      "Phase val, Epoch [2/2], Step [40/312], Loss: 0.4031, Accuracy: 79.69%\n",
      "val Epoch loss: 0.3946\n"
     ]
    }
   ],
   "source": [
    "#dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader, \"test\": test_dataloader}\n",
    "datasets_length = {\"train\": len(train_dataloader), \"val\": len(val_dataloader), \"test\": len(test_dataloader)}\n",
    "phases = [\"train\", \"val\"]\n",
    "\n",
    "#number of iterations:\n",
    "total_step = len(train_dataloader) \n",
    "\n",
    "#for tracking the loss and accuracy of our model:\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "      for phase in phases:\n",
    "            if phase == \"train\":\n",
    "                #optimizer = scheduler(optimizer, epoch)\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.\n",
    "\n",
    "            for i, data in enumerate(dataloaders[phase]):\n",
    "                images = data[\"image\"].to(device)\n",
    "                labels = data[\"label\"].to(device)        \n",
    "\n",
    "                # run the forward pass\n",
    "                outputs = model(images)\n",
    "                #calculate loss between predicted and labels:\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "                # run the backward pass (backpropagation) and perform adams optimization as specified above\n",
    "                optimizer.zero_grad() # here the gradients are zeroed before backprop starts\n",
    "                \n",
    "                if phase == \"train\":\n",
    "                    loss.backward() # calcs the gradients\n",
    "                    optimizer.step() #adam optimizer training step\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #track the accuracy\n",
    "                total = labels.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1) #richtig??\n",
    "                correct = (predicted == labels).sum().item()\n",
    "                acc_list.append(correct / total)\n",
    "\n",
    "                #print out current results after another 100 iterations are done:\n",
    "                if (i+1) % 10 == 0: # TODO: change back to 100\n",
    "                    print ('Phase {}, Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%' \n",
    "                           .format(phase, epoch+1, NUM_EPOCHS, i+1, total_step, loss.item(), (correct / total) * 100))\n",
    "\n",
    "            #epoch loss zeug:\n",
    "            epoch_loss = running_loss / datasets_length[phase]\n",
    "            print('{} Epoch loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "#Save our model to new directory\n",
    "#to keep track of different models created we append a timestamp to the model name\n",
    "time = datetime.now(tz=None)\n",
    "timestamp = str(time.year) + \"_\" + str(time.month) + \"_\" + str(time.day) + \"_\" + str(time.hour)+ \":\" + str(time.minute)+ \":\" + str(time.second)\n",
    "\n",
    "new_model_new_dir = OUTPUT_PATH + timestamp\n",
    "os.mkdir(new_model_new_dir)\n",
    "curr_model_output_path = new_model_new_dir + \"/model_version_\" + timestamp + \".pth\"\n",
    "torch.save(model.state_dict(), curr_model_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finaler Versuch??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neuer versuch nach pytorch tutorial:\n",
    "phases = [\"train\", \"val\"]\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=2):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            #initialize tp etc. with zero:\n",
    "            tn, fn, fp, tp = 0, 0, 0, 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for i, data in enumerate(dataloaders[phase]):\n",
    "                inputs = data[\"image\"].to(device)\n",
    "                labels = data[\"label\"].to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                 \n",
    "                        \n",
    "                #calculate true positives etc.:\n",
    "                for pred, lab in zip(predictions, labels):\n",
    "                    if(pred == 0 and lab == 0):\n",
    "                        tn += 1\n",
    "                    if(pred == 0 and lab == 1):\n",
    "                        fn += 1\n",
    "                    if(pred == 1 and lab == 0):\n",
    "                        fp += 1\n",
    "                    if(pred ==1 and lab ==1):\n",
    "                        tp += 1\n",
    "                \n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "            \n",
    "                correct = (predictions == labels).sum().item()\n",
    "                total = labels.size(0)\n",
    "                nr_iterations = len(dataloaders[phase])\n",
    "            \n",
    "                #print out current results after another 100 iterations are done:\n",
    "                if (i+1) % 10 == 0: # TODO: change back to 100\n",
    "                    print ('Phase {}, Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%' \n",
    "                           .format(phase, epoch+1, NUM_EPOCHS, i+1, nr_iterations, loss.item(), (correct / total) * 100))\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            #calculate precision, recall and f1:\n",
    "            epoch_precision = tp / (tp + fp)\n",
    "            epoch_recall = tp / (tp + fn)\n",
    "            epoch_f1 = 2 * ((epoch_precision * epoch_recall) / (epoch_precision + epoch_recall))\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Precision: {:.4f} Recall: {:.4f} F1: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_precision, epoch_recall, epoch_f1))\n",
    "\n",
    "            # deep copy the model (bisher mit accuracy)\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "            #    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            #alternativ mit f1:\n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('Best val F1: {:4f}'.format(best_f1))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "Phase train, Epoch [1/2], Step [10/312], Loss: 0.3876, Accuracy: 81.25%\n",
      "Phase train, Epoch [1/2], Step [20/312], Loss: 0.3374, Accuracy: 84.38%\n",
      "Phase train, Epoch [1/2], Step [30/312], Loss: 0.4919, Accuracy: 75.00%\n",
      "Phase train, Epoch [1/2], Step [40/312], Loss: 0.3496, Accuracy: 84.38%\n",
      "Phase train, Epoch [1/2], Step [50/312], Loss: 0.2629, Accuracy: 85.94%\n",
      "Phase train, Epoch [1/2], Step [60/312], Loss: 0.3837, Accuracy: 81.25%\n",
      "Phase train, Epoch [1/2], Step [70/312], Loss: 0.4603, Accuracy: 79.69%\n",
      "Phase train, Epoch [1/2], Step [80/312], Loss: 0.2534, Accuracy: 92.19%\n",
      "Phase train, Epoch [1/2], Step [90/312], Loss: 0.3710, Accuracy: 84.38%\n",
      "Phase train, Epoch [1/2], Step [100/312], Loss: 0.3186, Accuracy: 87.50%\n",
      "Phase train, Epoch [1/2], Step [110/312], Loss: 0.3609, Accuracy: 89.06%\n",
      "Phase train, Epoch [1/2], Step [120/312], Loss: 0.3066, Accuracy: 85.94%\n",
      "Phase train, Epoch [1/2], Step [130/312], Loss: 0.4145, Accuracy: 89.06%\n",
      "Phase train, Epoch [1/2], Step [140/312], Loss: 0.4232, Accuracy: 82.81%\n",
      "Phase train, Epoch [1/2], Step [150/312], Loss: 0.5188, Accuracy: 75.00%\n",
      "Phase train, Epoch [1/2], Step [160/312], Loss: 0.2527, Accuracy: 92.19%\n",
      "Phase train, Epoch [1/2], Step [170/312], Loss: 0.3797, Accuracy: 82.81%\n",
      "Phase train, Epoch [1/2], Step [180/312], Loss: 0.3303, Accuracy: 87.50%\n",
      "Phase train, Epoch [1/2], Step [190/312], Loss: 0.3085, Accuracy: 90.62%\n",
      "Phase train, Epoch [1/2], Step [200/312], Loss: 0.2801, Accuracy: 89.06%\n",
      "Phase train, Epoch [1/2], Step [210/312], Loss: 0.4090, Accuracy: 78.12%\n",
      "Phase train, Epoch [1/2], Step [220/312], Loss: 0.2758, Accuracy: 89.06%\n",
      "Phase train, Epoch [1/2], Step [230/312], Loss: 0.3377, Accuracy: 81.25%\n",
      "Phase train, Epoch [1/2], Step [240/312], Loss: 0.4094, Accuracy: 82.81%\n",
      "Phase train, Epoch [1/2], Step [250/312], Loss: 0.3165, Accuracy: 84.38%\n",
      "Phase train, Epoch [1/2], Step [260/312], Loss: 0.4576, Accuracy: 79.69%\n",
      "Phase train, Epoch [1/2], Step [270/312], Loss: 0.3049, Accuracy: 89.06%\n",
      "Phase train, Epoch [1/2], Step [280/312], Loss: 0.3037, Accuracy: 92.19%\n",
      "Phase train, Epoch [1/2], Step [290/312], Loss: 0.4423, Accuracy: 76.56%\n",
      "Phase train, Epoch [1/2], Step [300/312], Loss: 0.3523, Accuracy: 85.94%\n",
      "Phase train, Epoch [1/2], Step [310/312], Loss: 0.3061, Accuracy: 89.06%\n",
      "train Loss: 0.3636 Acc: 0.8403 Precision: 0.7561 Recall: 0.6795 F1: 0.7157\n",
      "Phase val, Epoch [1/2], Step [10/46], Loss: 0.4605, Accuracy: 82.81%\n",
      "Phase val, Epoch [1/2], Step [20/46], Loss: 0.5174, Accuracy: 75.00%\n",
      "Phase val, Epoch [1/2], Step [30/46], Loss: 0.8947, Accuracy: 53.12%\n",
      "Phase val, Epoch [1/2], Step [40/46], Loss: 0.7843, Accuracy: 57.81%\n",
      "val Loss: 0.5435 Acc: 0.7240 Precision: 0.4189 Recall: 0.9209 F1: 0.5758\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "Phase train, Epoch [2/2], Step [10/312], Loss: 0.2558, Accuracy: 92.19%\n",
      "Phase train, Epoch [2/2], Step [20/312], Loss: 0.4058, Accuracy: 82.81%\n",
      "Phase train, Epoch [2/2], Step [30/312], Loss: 0.3322, Accuracy: 90.62%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-812857867a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#training ausführen:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-270a99aa8d48>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-d7afc074f211>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvLayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#print(\"after conv1: \" + str(out.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvLayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(\"after conv2: \" + str(out.shape)) # after conv2: torch.Size([32, 32, 34, 34])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# what does reshape do? -> flatten. why befor drop_out?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training ausführen:\n",
    "\n",
    "model = train_model(model, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "#Save our model to new directory\n",
    "#to keep track of different models created we append a timestamp to the model name\n",
    "time = datetime.now(tz=None)\n",
    "timestamp = str(time.year) + \"_\" + str(time.month) + \"_\" + str(time.day) + \"_\" + str(time.hour)+ \":\" + str(time.minute)+ \":\" + str(time.second)\n",
    "\n",
    "new_model_new_dir = OUTPUT_PATH + timestamp\n",
    "os.mkdir(new_model_new_dir)\n",
    "curr_model_output_path = new_model_new_dir + \"/model_version_\" + timestamp + \".pth\"\n",
    "torch.save(model.state_dict(), curr_model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save loss and accuracy from training to same directory as model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save loss and acc after training: \n",
    "#TODO: compare later with results from testing?\n",
    "\n",
    "len_loss = len(loss_list) # nrEpoch * nrIterationsPerEpoch\n",
    "#print(len_loss)\n",
    "\n",
    "#create pandas df with loss and acc from training:\n",
    "loss_acc_stats = pd.DataFrame(loss_list, columns=[\"train_loss\"])\n",
    "loss_acc_stats[\"train_acc\"] = acc_list\n",
    "new_model_new_dir\n",
    "\n",
    "path = new_model_new_dir + \"/loss_acc_stats.json\"\n",
    "loss_acc_stats.to_json(path)\n",
    "\n",
    "#loss_acc_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load former loss and accuracy in again (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21940, 2)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load loss_acc_stats in again if needed:\n",
    "loaded_loss_acc_stats = pd.read_json(path)\n",
    "loaded_loss_acc_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of training loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Train Accuracy')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "ax[0].plot(np.array(loaded_loss_acc_stats[\"train_loss\"]))\n",
    "ax[1].plot(np.array(loaded_loss_acc_stats[\"train_acc\"]))\n",
    "ax[0].set_xlabel(\"Steps\")\n",
    "ax[0].set_ylabel(\"Train Loss\")\n",
    "ax[1].set_xlabel(\"Steps\")\n",
    "ax[1].set_ylabel(\"Train Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First load in the model from directory\n",
    "\n",
    "When the training is done we can just upload our saved model from the directory we saved it to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO ist alles wichtige in dem model drin? \n",
    "#see: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "\n",
    "#JUST FOR NOW:\n",
    "curr_model_output_path = \"saved_models/model_version_2020_1_11_22:12:11/model_version_2020_1_11_22:12:11.pth\"\n",
    "\n",
    "#test with current model:\n",
    "model_to_test = ThreeLayerCNN()\n",
    "model_to_test.load_state_dict(torch.load(curr_model_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO was ist mit unseren validierungsdaten? noch nicht genutzt bisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy on the test image patches\n",
    "\n",
    "Setting the model to evaluation mode by calling model.eval() disables drop-out and batch normalization layers in our model.\n",
    "The function call torch.no_grad() on the other hand disables the autograd functionality in our model. It is not needed for evaluation. Further it speeds up the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.data: tensor([[ 1.1094, -0.1617],\n",
      "        [ 0.4206,  0.1669],\n",
      "        [-1.6465,  2.0683],\n",
      "        [ 1.4617, -1.7075],\n",
      "        [-0.0880, -0.6063],\n",
      "        [-0.2227, -0.6956],\n",
      "        [ 2.1606, -2.3986],\n",
      "        [-0.1660,  1.9530],\n",
      "        [ 1.3142, -1.3870],\n",
      "        [ 0.5143, -0.2460],\n",
      "        [ 0.9425, -0.9261],\n",
      "        [-0.8838,  0.6661],\n",
      "        [ 0.3153, -1.4051],\n",
      "        [ 7.5904, -2.7344],\n",
      "        [-0.0766, -0.1923],\n",
      "        [ 0.9657,  0.1103],\n",
      "        [-0.5024,  1.3522],\n",
      "        [ 0.0676, -1.9019],\n",
      "        [-0.7425,  0.8384],\n",
      "        [ 0.2795, -0.5575],\n",
      "        [ 2.0539, -2.5005],\n",
      "        [ 1.8353, -2.5913],\n",
      "        [ 1.9205, -0.0184],\n",
      "        [ 4.5180, -3.0931],\n",
      "        [ 2.0695, -1.3346],\n",
      "        [ 2.4619, -0.4327],\n",
      "        [-0.8207, -1.3976],\n",
      "        [ 0.6544,  0.5012],\n",
      "        [-0.5590,  0.9989],\n",
      "        [ 0.7680, -1.1091],\n",
      "        [ 1.2894, -0.8370],\n",
      "        [-0.1474,  0.0534],\n",
      "        [-1.0354,  1.0898],\n",
      "        [ 0.2773,  0.8918],\n",
      "        [ 1.4618, -1.3260],\n",
      "        [ 0.8121, -0.0293],\n",
      "        [ 1.6232, -3.5971],\n",
      "        [ 0.8944, -1.0618],\n",
      "        [ 1.4261, -1.7906],\n",
      "        [ 0.7955, -1.7528],\n",
      "        [ 0.4345,  1.0342],\n",
      "        [ 2.0801, -0.9775],\n",
      "        [ 0.1371,  1.7844],\n",
      "        [ 0.8915, -1.2949],\n",
      "        [ 0.3332, -1.6352],\n",
      "        [ 0.4475, -0.5166],\n",
      "        [ 0.4042, -0.9004],\n",
      "        [ 4.8288, -3.4728],\n",
      "        [ 1.6148, -1.9681],\n",
      "        [-0.9465,  2.2104],\n",
      "        [ 0.8829, -0.4947],\n",
      "        [ 0.3765, -0.6469],\n",
      "        [ 1.5396, -1.7466],\n",
      "        [ 1.1757,  0.0600],\n",
      "        [ 1.6320, -0.6175],\n",
      "        [-0.5691,  0.9615],\n",
      "        [ 2.4181, -2.0633],\n",
      "        [ 1.1989, -0.3464],\n",
      "        [-0.7675,  0.5045],\n",
      "        [ 1.0156, -1.3726],\n",
      "        [ 0.3922, -0.5362],\n",
      "        [-1.1117,  1.3190],\n",
      "        [ 1.5696, -0.7625],\n",
      "        [-0.0440,  0.5064]])\n",
      "softmax: \n",
      "tensor([[7.8092e-01, 2.1908e-01],\n",
      "        [5.6307e-01, 4.3693e-01],\n",
      "        [2.3781e-02, 9.7622e-01],\n",
      "        [9.5966e-01, 4.0338e-02],\n",
      "        [6.2675e-01, 3.7325e-01],\n",
      "        [6.1605e-01, 3.8395e-01],\n",
      "        [9.8964e-01, 1.0362e-02],\n",
      "        [1.0726e-01, 8.9274e-01],\n",
      "        [9.3710e-01, 6.2903e-02],\n",
      "        [6.8143e-01, 3.1857e-01],\n",
      "        [8.6630e-01, 1.3370e-01],\n",
      "        [1.7511e-01, 8.2489e-01],\n",
      "        [8.4817e-01, 1.5183e-01],\n",
      "        [9.9997e-01, 3.2808e-05],\n",
      "        [5.2889e-01, 4.7111e-01],\n",
      "        [7.0170e-01, 2.9830e-01],\n",
      "        [1.3533e-01, 8.6467e-01],\n",
      "        [8.7755e-01, 1.2245e-01],\n",
      "        [1.7067e-01, 8.2933e-01],\n",
      "        [6.9783e-01, 3.0217e-01],\n",
      "        [9.8959e-01, 1.0411e-02],\n",
      "        [9.8819e-01, 1.1814e-02],\n",
      "        [8.7424e-01, 1.2576e-01],\n",
      "        [9.9951e-01, 4.9468e-04],\n",
      "        [9.6783e-01, 3.2166e-02],\n",
      "        [9.4758e-01, 5.2422e-02],\n",
      "        [6.4034e-01, 3.5966e-01],\n",
      "        [5.3824e-01, 4.6176e-01],\n",
      "        [1.7396e-01, 8.2604e-01],\n",
      "        [8.6728e-01, 1.3272e-01],\n",
      "        [8.9344e-01, 1.0656e-01],\n",
      "        [4.4998e-01, 5.5002e-01],\n",
      "        [1.0666e-01, 8.9334e-01],\n",
      "        [3.5103e-01, 6.4897e-01],\n",
      "        [9.4201e-01, 5.7987e-02],\n",
      "        [6.9876e-01, 3.0124e-01],\n",
      "        [9.9462e-01, 5.3770e-03],\n",
      "        [8.7613e-01, 1.2387e-01],\n",
      "        [9.6146e-01, 3.8539e-02],\n",
      "        [9.2746e-01, 7.2543e-02],\n",
      "        [3.5442e-01, 6.4558e-01],\n",
      "        [9.5511e-01, 4.4888e-02],\n",
      "        [1.6146e-01, 8.3854e-01],\n",
      "        [8.9902e-01, 1.0098e-01],\n",
      "        [8.7743e-01, 1.2257e-01],\n",
      "        [7.2393e-01, 2.7607e-01],\n",
      "        [7.8661e-01, 2.1339e-01],\n",
      "        [9.9975e-01, 2.4806e-04],\n",
      "        [9.7296e-01, 2.7044e-02],\n",
      "        [4.0819e-02, 9.5918e-01],\n",
      "        [7.9861e-01, 2.0139e-01],\n",
      "        [7.3563e-01, 2.6437e-01],\n",
      "        [9.6395e-01, 3.6046e-02],\n",
      "        [7.5318e-01, 2.4682e-01],\n",
      "        [9.0461e-01, 9.5389e-02],\n",
      "        [1.7790e-01, 8.2210e-01],\n",
      "        [9.8881e-01, 1.1191e-02],\n",
      "        [8.2423e-01, 1.7577e-01],\n",
      "        [2.1893e-01, 7.8107e-01],\n",
      "        [9.1593e-01, 8.4074e-02],\n",
      "        [7.1673e-01, 2.8327e-01],\n",
      "        [8.0858e-02, 9.1914e-01],\n",
      "        [9.1150e-01, 8.8499e-02],\n",
      "        [3.6577e-01, 6.3423e-01]])\n",
      "torch.max(outputs.data, 1): torch.return_types.max(\n",
      "values=tensor([ 1.1094,  0.4206,  2.0683,  1.4617, -0.0880, -0.2227,  2.1606,  1.9530,\n",
      "         1.3142,  0.5143,  0.9425,  0.6661,  0.3153,  7.5904, -0.0766,  0.9657,\n",
      "         1.3522,  0.0676,  0.8384,  0.2795,  2.0539,  1.8353,  1.9205,  4.5180,\n",
      "         2.0695,  2.4619, -0.8207,  0.6544,  0.9989,  0.7680,  1.2894,  0.0534,\n",
      "         1.0898,  0.8918,  1.4618,  0.8121,  1.6232,  0.8944,  1.4261,  0.7955,\n",
      "         1.0342,  2.0801,  1.7844,  0.8915,  0.3332,  0.4475,  0.4042,  4.8288,\n",
      "         1.6148,  2.2104,  0.8829,  0.3765,  1.5396,  1.1757,  1.6320,  0.9615,\n",
      "         2.4181,  1.1989,  0.5045,  1.0156,  0.3922,  1.3190,  1.5696,  0.5064]),\n",
      "indices=tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1]))\n",
      "predicted: tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laureslinuxes/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-9ab77aec8c54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#iterate over predicted and labels to get tp, fp, tn, tf:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mtn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    425\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#set the model to evaluation mode\n",
    "model_to_test.eval()\n",
    "\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    tn = 0 \n",
    "    fn = 0 \n",
    "    fp = 0 \n",
    "    tp = 0 \n",
    "    total = 0\n",
    "    \n",
    "    #nur für jetzt:\n",
    "    a = 0\n",
    "    d = 0\n",
    "    \n",
    "    #for images, labels in test_dataloader:\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "        \n",
    "        outputs = model_to_test(images)\n",
    "        if(a == 0):\n",
    "            print(\"outputs.data: \" + str(outputs.data))\n",
    "            print(\"softmax: \")\n",
    "            print(nn.functional.softmax(outputs)) # HIER KOMMEN DIE WAHRSCHEINLICHKEITEN RAUS\n",
    "            a = 1\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        if(d == 0):\n",
    "            print(\"torch.max(outputs.data, 1): \" + str(torch.max(outputs.data, 1)))\n",
    "            print(\"predicted: \" + str(predicted))\n",
    "            d = 1\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        #iterate over predicted and labels to get tp, fp, tn, tf:\n",
    "        for pred, lab in zip(predicted, labels):\n",
    "            if(pred == 0 and lab == 0):\n",
    "                tn += 1\n",
    "            if(pred == 0 and lab == 1):\n",
    "                fn += 1\n",
    "            if(pred == 1 and lab == 0):\n",
    "                fp += 1\n",
    "            if(pred ==1 and lab ==1):\n",
    "                tp += 1\n",
    "        \n",
    "            #befill confusion_matrix:\n",
    "            confusion_matrix[lab.long(), pred.long()] += 1\n",
    "        \n",
    "    \n",
    "    print('Anzahl Testdaten gesamt: ' + str(total))\n",
    "    print('Richtig vorhergesagt (TN und TP): ' + str(correct))\n",
    "    print('Falsch vorhergesagt (FN und FP): ' + str(total - correct))\n",
    "    print('True negative: ' + str(tn))\n",
    "    print('False negative: ' + str(fn))\n",
    "    print('False positive: ' + str(fp))\n",
    "    print('True positive: ' + str(tp))\n",
    "    print('Test Accuracy of the model on the 43313 test image patches: {} %'.format((correct / total) * 100))\n",
    "\n",
    "# Save the model checkpoint\n",
    "#curr_model_eval_path = new_model_new_dir + \"/model_eval_\" + timestamp + \".ckpt\"\n",
    "#torch.save(model_to_test.state_dict(), curr_model_eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24930.,  5335.],\n",
      "        [ 3195.,  9853.]])\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted: 0</th>\n",
       "      <th>Predicted: 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24930.0</td>\n",
       "      <td>5335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3195.0</td>\n",
       "      <td>9853.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted: 0  Predicted: 1\n",
       "0       24930.0        5335.0\n",
       "1        3195.0        9853.0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#versuch umwandlung in numpy array, dann in pandas dataframe für einen plot (bisher nicht erfolgreich)\n",
    "matrix_as_arr = np.array(confusion_matrix)\n",
    "\n",
    "matrix_as_df = pd.DataFrame({'Predicted: 0': matrix_as_arr[:,0], 'Predicted: 1': matrix_as_arr[:,1]})\n",
    "matrix_as_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO vielleicht richtig visualisieren oder too much? mir egal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate precision and recall for our test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "\n",
      "Precision tells us how many of the overall predicted positive are actually true positive ones. Precision is a good measure to tell us if the cost of false positive is high.\n",
      "For our case: From the 15188 image patches that we predicted as having IDC, only 9853 patches actually contain IDC.\n",
      "Hence, only 64.87358440874374% are being predicted right.\n",
      "Depending on which patches belong to which image from which patient, we would have told quite some patients that they have IDC, even though they don't have it.\n",
      "\n",
      "\n",
      "Recall:\n",
      "\n",
      "Recall calculates how many of the actual positives from our model we captured through labeling them as positive. This metric tells us whether there is a high cost associated with false negative.\n",
      "For our case: From all the 13048 image patches that actually show IDC, we only predicted 9853 image patches correctly.\n",
      "Hence, 75.51348865726548% are being predicted right.\n",
      "Again depending on which patches belong to which image from which patient we would have told some patients they don't have IDC even though they have breast cancer.\n"
     ]
    }
   ],
   "source": [
    "#precision\n",
    "precision = tp / (tp + fp) #precision denominator: total predicted positive\n",
    "\n",
    "#recall\n",
    "recall = tp / (tp + fn) #recall denominator: total actual positive\n",
    "\n",
    "print(\"Precision:\")\n",
    "print()\n",
    "print(\"Precision tells us how many of the overall predicted positive are actually true positive ones. Precision is a good measure to tell us if the cost of false positive is high.\")\n",
    "print(\"For our case: From the \" + str(tp + fp) + \" image patches that we predicted as having IDC, only \" +\n",
    "      str(tp) + \" patches actually contain IDC.\")\n",
    "print(\"Hence, only \" + str(precision * 100) + \"% are being predicted right.\")\n",
    "print(\"Depending on which patches belong to which image from which patient, we would have told quite some patients that they have IDC, even though they don't have it.\")\n",
    "print()\n",
    "print()\n",
    "print(\"Recall:\")\n",
    "print()\n",
    "print(\"Recall calculates how many of the actual positives from our model we captured through labeling them as positive. This metric tells us whether there is a high cost associated with false negative.\")\n",
    "print(\"For our case: From all the \" + str(tp + fn) + \" image patches that actually show IDC, we only predicted \" +\n",
    "     str(tp) + \" image patches correctly.\")\n",
    "print(\"Hence, \" + str(recall * 100) + \"% are being predicted right.\")\n",
    "print(\"Again depending on which patches belong to which image from which patient we would have told some patients they don't have IDC even though they have breast cancer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate F1 measure for our test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is needed when seeking a balance between precision and recall. Even though accuracy is a similar measurement, the accuracy can be influenced by a large number of true negatives  (saying an uneven class distribution).\n",
      "\n",
      "In our case considering the test dataset, from the overall 43313 patches, 30265 patches contain no IDC, while 13048 contain IDC.\n",
      "So, 69.87509523699582% of the patches contain no IDC and 30.124904763004178% have IDC in them.\n",
      "As shown above the number of true negatives (24930) is quite high in our test data.\n",
      "\n",
      "The F1 score for our test data is: 0.6979033857486896\n"
     ]
    }
   ],
   "source": [
    "#calc f1\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "all_patches = tn + fp + fn + tp\n",
    "\n",
    "print(\"F1 score is needed when seeking a balance between precision and recall. \" +\n",
    "      \"Even though accuracy is a similar measurement, the accuracy can be influenced by a large number of true negatives \" +\n",
    "      \" (saying an uneven class distribution).\")\n",
    "print()\n",
    "print(\"In our case considering the test dataset, from the overall \" + str(all_patches) + \" patches, \" +\n",
    "      str(tn + fp) + \" patches contain no IDC, while \" + str(fn + tp) + \" contain IDC.\")\n",
    "print(\"So, \" + str(((tn + fp) / all_patches) * 100) + \"% of the patches contain no IDC and \" \n",
    "      + str(((fn + tp) / all_patches) * 100) + \"% have IDC in them.\")\n",
    "print(\"As shown above the number of true negatives (\" + str(tn) + \") is quite high in our test data.\")\n",
    "print()\n",
    "print(\"The F1 score for our test data is: \" + str(f1))\n",
    "#TODO noch mehr erläutern???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "## TODO: Hier wird gerade nur ein einfacher plot von accuracy und loss gemacht, noch mehr machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO vervollständigen; mehr visualisierung etc.\n",
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='Model results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save bokeh plot needs installation:\n",
    "#pip install selenium\n",
    "#this didn't work:\n",
    "#npm install -g phantomjs-prebuilt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export plot and save to directory:\n",
    "#from bokeh.io import export_png\n",
    "#file_path = new_model_new_dir + \"bokeh_plot.png\"\n",
    "#export_png(p, filename=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BreastCancer",
   "language": "python",
   "name": "breastcancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

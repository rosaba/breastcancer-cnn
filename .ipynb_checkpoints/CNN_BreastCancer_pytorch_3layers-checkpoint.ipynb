{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConV Network: Setup, training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # version 1.3.1\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CyclicLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# data augmentation\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "from PIL import ImageFilter\n",
    "\n",
    "# Split arrays or matrices into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# remove if not needed because augmentation is already applied \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# INSTALL tqdm for jupyter lab:\n",
    "# 1. pip install tqdm==4.36.1\n",
    "# 2. pip install ipywidgets\n",
    "# 3. jupyter nbextension enable --py widgetsnbextension\n",
    "# 4. jupyter labextension install @jupyter-widgets/jupyterlab-manager (installed nodejs and npm needed)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#visualization at the end:\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary data from file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_train_df = pd.read_json(\"dataframes/final_train_df.json\")\n",
    "loaded_val_df = pd.read_json(\"dataframes/val_df.json\")\n",
    "loaded_test_df = pd.read_json(\"dataframes/test_df.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>801</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>801</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>851</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>601</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10258</td>\n",
       "      <td>data/breast-histopathology-images/IDC_regular_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280827</th>\n",
       "      <td>9173</td>\n",
       "      <td>data/train_class1_augmented/9173_idx5_x2301_y1...</td>\n",
       "      <td>1</td>\n",
       "      <td>2301</td>\n",
       "      <td>1601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280828</th>\n",
       "      <td>13693</td>\n",
       "      <td>data/train_class1_augmented/13693_idx5_x551_y1...</td>\n",
       "      <td>1</td>\n",
       "      <td>551</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280829</th>\n",
       "      <td>13402</td>\n",
       "      <td>data/train_class1_augmented/13402_idx5_x1451_y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1451</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280830</th>\n",
       "      <td>16165</td>\n",
       "      <td>data/train_class1_augmented/16165_idx5_x1401_y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1401</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280831</th>\n",
       "      <td>14157</td>\n",
       "      <td>data/train_class1_augmented/14157_idx5_x1451_y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1451</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280832 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        patient_id                                         image_path  label  \\\n",
       "0            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "1            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "2            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "3            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "4            10258  data/breast-histopathology-images/IDC_regular_...      0   \n",
       "...            ...                                                ...    ...   \n",
       "280827        9173  data/train_class1_augmented/9173_idx5_x2301_y1...      1   \n",
       "280828       13693  data/train_class1_augmented/13693_idx5_x551_y1...      1   \n",
       "280829       13402  data/train_class1_augmented/13402_idx5_x1451_y...      1   \n",
       "280830       16165  data/train_class1_augmented/16165_idx5_x1401_y...      1   \n",
       "280831       14157  data/train_class1_augmented/14157_idx5_x1451_y...      1   \n",
       "\n",
       "           x     y  \n",
       "0        801  1151  \n",
       "1        801   951  \n",
       "2        851   651  \n",
       "3        601   951  \n",
       "4       1001   851  \n",
       "...      ...   ...  \n",
       "280827  2301  1601  \n",
       "280828   551  1551  \n",
       "280829  1451  1001  \n",
       "280830  1401  1501  \n",
       "280831  1451  1251  \n",
       "\n",
       "[280832 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show example file:\n",
    "loaded_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting hyperparameters for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #bisher 32; beim model von letzter nacht 128\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.001 #beim model von letzter nacht 0.002\n",
    "NUM_EPOCHS = 5 #vorher 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directory for saving the model after training (only execute once):\n",
    "#os.mkdir(\"saved_models\")\n",
    "\n",
    "OUTPUT_PATH = \"saved_models/model_version_\"\n",
    "MODEL_EVAL_PATH = \"saved_models/model_eval_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data\n",
    "\n",
    "This allows us to manipulate our three datasets in a specified way.\n",
    "\n",
    "In our first notebook on data preprocessing we already augmented the image patches in the training set with class 1 to boost their size. The goal was to reach an equal size for both classes 0 and 1. Here we still add another transformation on the training data (horizontal and vertical flipping plus rotation) to boost the overall size of our training data (now both classes).\n",
    "\n",
    "Further we need to convert the input data to PyTorch tensor.\n",
    "\n",
    "Additionally we add normalization. Neural networks train better when the input data is normalized, so that the data ranges from -1 to 1 or from 0 to 1. For that we need to compute the standard deviation and the mean of our training data and apply them to our train_transform. \n",
    "Note: for each input channel (here: 3) we need to apply the according mean and standard deviation.\n",
    "\n",
    "The last two transformations count also for the validation and test datasets, whereas the first mentioned transformations only account for our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean and standard deviation of our three datasets\n",
    "\n",
    "To hand the right parameters for mean and standard deviation to our my_transform function further down we first create three simple dataloader objects with our corresponding datasets to calculate the mean and standard deviation respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple transform method; only for purpose of calculation mean and standard deviation\n",
    "def transform_to_tensor():\n",
    "    train_trans = [transforms.Resize((50, 50)), transforms.ToTensor()]\n",
    "    return transforms.Compose(train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test train_loader to calculate mean and standard dev on:\n",
    "train_set = BreastCancerDataset(loaded_train_df, transform=transform_to_tensor())\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "\n",
    "for _,data in enumerate(train_loader):\n",
    "    images = data[\"image\"].to(device)\n",
    "    batch_samples = images.size(0) \n",
    "    data = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    nb_samples += batch_samples\n",
    "    \n",
    "mean /= nb_samples\n",
    "std /= nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are the mean values over our three dimensions in the train dataset:\n",
      "tensor([0.7718, 0.6068, 0.7322])\n",
      "\n",
      "Those are the standard deviation values over our three dims in train dataset:\n",
      "tensor([0.0926, 0.1417, 0.1081])\n"
     ]
    }
   ],
   "source": [
    "print(\"Those are the mean values over our three dimensions in the train dataset:\")\n",
    "print(str(mean))\n",
    "print()\n",
    "print(\"Those are the standard deviation values over our three dims in train dataset:\")\n",
    "print(str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied in values calculated above so we don't have to execute the two cells more often:\n",
    "means_train = [0.7718, 0.6068, 0.7322]\n",
    "standard_dev_train = [0.0926, 0.1417, 0.1081]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean and standard deviation of our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test_loader to calculate mean and standard dev for the test dataset:\n",
    "test_set = BreastCancerDataset(loaded_test_df, transform=transform_to_tensor())\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = 0.\n",
    "test_std = 0.\n",
    "test_nb_samples = 0.\n",
    "\n",
    "for _,data in enumerate(test_loader):\n",
    "    images = data[\"image\"].to(device)\n",
    "    t_batch_samples = images.size(0) \n",
    "    data = images.view(t_batch_samples, images.size(1), -1)\n",
    "    test_mean += data.mean(2).sum(0)\n",
    "    test_std += data.std(2).sum(0)\n",
    "    test_nb_samples += t_batch_samples\n",
    "    \n",
    "test_mean /= test_nb_samples\n",
    "test_std /= test_nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are the mean values over our three dimensions in the test dataset:\n",
      "tensor([0.8208, 0.6423, 0.7406])\n",
      "\n",
      "Those are the standard deviation values over our three dims in test dataset:\n",
      "tensor([0.0892, 0.1410, 0.1037])\n"
     ]
    }
   ],
   "source": [
    "print(\"Those are the mean values over our three dimensions in the test dataset:\")\n",
    "print(str(test_mean))\n",
    "print()\n",
    "print(\"Those are the standard deviation values over our three dims in test dataset:\")\n",
    "print(str(test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied in values:\n",
    "#test_mean = [0.8208, 0.6423, 0.7406]\n",
    "#test_std = [0.0892, 0.1410, 0.1037]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean and standard deviation of our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create val_loader to calculate mean and standard dev for the validation dataset:\n",
    "val_set = BreastCancerDataset(loaded_val_df, transform=transform_to_tensor())\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mean = 0.\n",
    "val_std = 0.\n",
    "val_nb_samples = 0.\n",
    "\n",
    "for _,data in enumerate(val_loader):\n",
    "    images = data[\"image\"].to(device)\n",
    "    v_batch_samples = images.size(0) \n",
    "    data = images.view(v_batch_samples, images.size(1), -1)\n",
    "    val_mean += data.mean(2).sum(0)\n",
    "    val_std += data.std(2).sum(0)\n",
    "    val_nb_samples += v_batch_samples\n",
    "    \n",
    "val_mean /= val_nb_samples\n",
    "val_std /= val_nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are the mean values over our three dimensions in the validation dataset:\n",
      "tensor([0.8031, 0.6202, 0.7270])\n",
      "\n",
      "Those are the standard deviation values over our three dims in validation dataset:\n",
      "tensor([0.0938, 0.1420, 0.1049])\n"
     ]
    }
   ],
   "source": [
    "print(\"Those are the mean values over our three dimensions in the validation dataset:\")\n",
    "print(str(val_mean))\n",
    "print()\n",
    "print(\"Those are the standard deviation values over our three dims in validation dataset:\")\n",
    "print(str(val_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied in values:\n",
    "#val_mean = [0.8031, 0.6202, 0.7270]\n",
    "#val_std = [0.0938, 0.1420, 0.1049]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following transform method will be used to manipulate our data sets for training and evaluation\n",
    "\n",
    "Here we add the calculated values for mean and standard deviation for purpose of normalization from above.\n",
    "For now we take the same values also for the validation and test data set, as we assume them to be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms to apply to our data:\n",
    "def my_transform(key=\"train_transform\"):\n",
    "    #boost class 1 in training set:\n",
    "    train_transform = [transforms.Resize((50, 50)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.RandomRotation(90), \n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.7718, 0.6068, 0.7322],std=[0.0926, 0.1417, 0.1081])]\n",
    "    #change values:\n",
    "    val_transform = [transforms.Resize((50, 50)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.8031, 0.6202, 0.7270],std=[0.0938, 0.1420, 0.1049])]\n",
    "    \n",
    "    test_transform = [transforms.Resize((50, 50)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.8208, 0.6423, 0.7406],std=[0.0892, 0.1410, 0.1037])]\n",
    "        \n",
    "    data_transforms = {'train_transform': transforms.Compose(train_transform),\n",
    "                       'val_transform': transforms.Compose(val_transform),\n",
    "                       'test_transform': transforms.Compose(test_transform)}\n",
    "    return data_transforms[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we create a class for our various datasets\n",
    "\n",
    "This class will be used to create our various datasets and subsequently be passed to our data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, transform=None):\n",
    "        self.states = df\n",
    "        self.transform=transform\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.states.patient_id.values[idx]\n",
    "        x_coord = self.states.x.values[idx]\n",
    "        y_coord = self.states.y.values[idx]\n",
    "        image_path = self.states.image_path.values[idx] \n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB') # try to convert to YUV instead of RGB later\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "         \n",
    "        label = np.int(self.states.label.values[idx])\n",
    "        return {\"image\": image,\n",
    "                \"label\": label,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"x\": x_coord,\n",
    "                \"y\": y_coord}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting our data sets (training, validation, testing) into a dataloader object\n",
    "\n",
    "Here we create our datasets and apply the transformations defined above in our my_transform method.\n",
    "After we befill our dataloader with the datasets.\n",
    "\n",
    "The data loader object in PyTorch provides a number of features which are useful for us in consuming training data.\n",
    "We can automatically shuffle the data. Further we can easily batch the data according to our defined BATCH_SIZE. PyTorch also enables us to load the data in parallel using multiprocessing. Data consumption becomes a lot more efficient this way.\n",
    "\n",
    "As we say later on in the training loop the PyTorch data loader object can be used as an iterator, i.e. use \"enumerate\" to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BreastCancerDataset(loaded_train_df, transform=my_transform(key=\"train_transform\"))\n",
    "val_dataset = BreastCancerDataset(loaded_val_df, transform=my_transform(key=\"val_transform\"))\n",
    "test_dataset = BreastCancerDataset(loaded_test_df, transform=my_transform(key=\"test_transform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\", \"test\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 280832, 'val': 37886, 'test': 43313}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the size of our three different data sets:\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader, \"test\": test_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training dataset has the size: 280832\n",
      "Divided by our batch_size of 64 this leads to 4388 iterations for each epoch while training.\n",
      "\n",
      "Our validation data set has the size: 37886\n",
      "Divided by our batch_size of 64 this leads to 591 iterations for each epoch for validation.\n",
      "\n",
      "Our test data set has the size: 43313\n",
      "Divided by our batch_size of 64 this leads to 677 iterations for each epoch for testing.\n"
     ]
    }
   ],
   "source": [
    "#meaning of number stored in dataloader: nr iterations for each epoch\n",
    "print(\"Our training dataset has the size: \" + str(dataset_sizes[\"train\"]))\n",
    "print(\"Divided by our batch_size of \" + str(BATCH_SIZE) + \" this leads to \" + \n",
    "      str(len(dataloaders[\"train\"])) + \" iterations for each epoch while training.\")\n",
    "print()\n",
    "print(\"Our validation data set has the size: \" + str(dataset_sizes[\"val\"]))\n",
    "print(\"Divided by our batch_size of \" + str(BATCH_SIZE) + \" this leads to \" + \n",
    "      str(len(dataloaders[\"val\"])) + \" iterations for each epoch for validation.\")\n",
    "print()\n",
    "print(\"Our test data set has the size: \" + str(dataset_sizes[\"test\"]))\n",
    "print(\"Divided by our batch_size of \" + str(BATCH_SIZE) + \" this leads to \" + \n",
    "      str(len(dataloaders[\"test\"])) + \" iterations for each epoch for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the architecture of our ConV neural network\n",
    "\n",
    "Here we define the architecture of our model. For that we create our own class which inherits from the nn.Module super class from PyTorch.\n",
    "For all functions that can be used from that super class see here: https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "With the nn.Sequential function we define the various layers in our network.\n",
    "Our network adapts a 3-layer CNN architecture employing 16, 32, 36.992 and 128 neurons for our first two convolutional layers and the two fully-connected layers respectively. We chose these numbers as we planned to orient our approach at the architecture chosen in this paper: 2014 ...\n",
    "Note: We had to add an additional fully-connected layer with 36.992 neurons. (Name reason TODO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the paper they chose a fixed convolutional kernel of size 8x8 and a pool kernel of size 2x2.\n",
    "\n",
    "\n",
    "We used ReLu as activation functions and maxPooling at the end of our convolutional layers to down-sample our data after each convolution by reducing the effective image size.\n",
    "Further we applied dropout to minimize the chance of overfitting, improve generalization and also improve the training time. In order to let the dropout layer do its job we have to flatten the input to that layer in the forward function. For further reasoning see here: https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2\n",
    "\n",
    "\n",
    "### TODO's:\n",
    "- Grund warum 2. fully connected layer nÃ¶tig? haben wir gerade ein 4-layer oder ein 3-layer netz?\n",
    "    (wegen anwendung von dropout und dessen reshaping/flattening nÃ¶tig? irgendwie so war es ja..)\n",
    "- vielleicht genauer begrÃ¼nden warum wir ReLu nehmen, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General method to calculate either the output size, the convolutional filter (kernel) size, the stride, or pooling:\n",
    "\n",
    "<math display=\"block\">\n",
    "  <msub>\n",
    "    <mi>W</mi>\n",
    "    <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "      <mi>o</mi>\n",
    "      <mi>u</mi>\n",
    "      <mi>t</mi>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <mo>=</mo>\n",
    "  <mfrac>\n",
    "    <mrow>\n",
    "      <mo stretchy=\"false\">(</mo>\n",
    "      <msub>\n",
    "        <mi>W</mi>\n",
    "        <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "          <mi>i</mi>\n",
    "          <mi>n</mi>\n",
    "        </mrow>\n",
    "      </msub>\n",
    "      <mo>&#x2013;</mo>\n",
    "      <mi>F</mi>\n",
    "      <mo>+</mo>\n",
    "      <mn>2</mn>\n",
    "      <mi>P</mi>\n",
    "      <mo stretchy=\"false\">)</mo>\n",
    "    </mrow>\n",
    "      <mi>/</mi>  \n",
    "    <mi>S</mi>\n",
    "  </mfrac>\n",
    "  <mo>+</mo>\n",
    "  <mn>1</mn>\n",
    "</math>\n",
    "\n",
    "- W in = width of the input\n",
    "- F = filter or kernel size\n",
    "- P = padding\n",
    "- S = stride\n",
    "- W out = output size\n",
    "\n",
    "The same formula accounts for the calculation of the height of the input and output, as we have the case of a symmetrical image size (height = width) and we also decided to take an equal number for height and width of the filter/kernel.\n",
    "\n",
    "### TODO \n",
    "- maybe show calculation why we chose which number for kernel size/stride etc or just argument with numbers from paper as done so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # ancestor constructor call\n",
    "        super(ThreeLayerCNN, self).__init__()\n",
    "        \n",
    "        # Conv Layer 1\n",
    "        self.convLayer1 = nn.Sequential( # TODO: calculate stride, padding\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=8, stride=1),\n",
    "            nn.BatchNorm2d(16), #NEU\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "        \n",
    "        # Conv Layer 2\n",
    "        self.convLayer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=8, stride=1),\n",
    "            nn.BatchNorm2d(32), #NEU\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "            \n",
    "        # Dropout: avoid overfitting. only for training\n",
    "        self.drop_out = nn.Dropout()        \n",
    "        \n",
    "        #Fully Connected Layer\n",
    "      #  self.fc1 = nn.Linear(128, 2) #      32, 34, 34 \n",
    "        self.fc1 = nn.Linear(32*34*34, 128)\n",
    "       # self.fc1 = nn.Linear(32*1*1, 128) # 32,1,1\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.convLayer1(x)\n",
    "        #print(\"after conv1: \" + str(out.shape))\n",
    "        out = self.convLayer2(out)\n",
    "        #print(\"after conv2: \" + str(out.shape)) # after conv2: torch.Size([32, 32, 34, 34])\n",
    "        # what does reshape do? -> flatten. why befor drop_out?\n",
    "       # out = out.reshape(out.size(0), -1) \n",
    "       # out = out.view(-1, 128)\n",
    "        out = out.view(-1, 32*34*34)\n",
    "       # out = out.view(-1, 32*1*1)\n",
    "        out = self.drop_out(out)\n",
    "        #print(\"after dropout: \" + str(out.shape))\n",
    "        out = self.fc1(out)\n",
    "        #print(\"after fc1: \" + str(out.shape))\n",
    "        out = self.fc2(out)\n",
    "        #print(\"after fc2: \" + str(out.shape))\n",
    "        \n",
    "        return out      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThreeLayerCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeLayerCNN(\n",
      "  (convLayer1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(8, 8), stride=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convLayer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(8, 8), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (drop_out): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=36992, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a loss function and an optimizer\n",
    "\n",
    "Before we train the model, we have to define our loss function and optimizer.\n",
    "We used PyTorchâ€™s CrossEntropyLoss() function.\n",
    "\n",
    "Notice: So far we haven't defined a softmax activation for the final fully-connected classification layer above.\n",
    "The reason is that the crossentropy function from PyTorch combines both a softmax activation together with a cross entropy loss function.\n",
    "\n",
    "After that we define an adam optimizer, which we pass our parameters that we want the optimizer to train. We also supply our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/4388], Loss: 0.2863, Accuracy: 87.50%\n",
      "Epoch [1/5], Step [200/4388], Loss: 0.1989, Accuracy: 95.31%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-adaa7eb46cff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# run the backward pass (backpropagation) and perform adams optimization as specified above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# here the gradients are zeroed before backprop starts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calcs the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#adam optimizer training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/BreastCancer-xbvsc4Xo/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#number of iterations:\n",
    "total_step = len(train_dataloader) \n",
    "\n",
    "#for tracking the loss and accuracy of our model:\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  \n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"label\"].to(device)        \n",
    "        \n",
    "        # run the forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        # run the backward pass (backpropagation) and perform adams optimization as specified above\n",
    "        optimizer.zero_grad() # here the gradients are zeroed before backprop starts\n",
    "        loss.backward() # calcs the gradients\n",
    "        optimizer.step() #adam optimizer training step\n",
    "        \n",
    "        #track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1) #richtig??\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        \n",
    "        #print out current results after another 100 iterations are done:\n",
    "        if (i+1) % 100 == 0: # TODO: change back to 100\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%' \n",
    "                   .format(epoch+1, NUM_EPOCHS, i+1, total_step, loss.item(), (correct / total) * 100))\n",
    "\n",
    "            \n",
    "#Save our model to new directory\n",
    "#to keep track of different models created we append a timestamp to the model name\n",
    "time = datetime.now(tz=None)\n",
    "timestamp = str(time.year) + \"_\" + str(time.month) + \"_\" + str(time.day) + \"_\" + str(time.hour)+ \":\" + str(time.minute)+ \":\" + str(time.second)\n",
    "\n",
    "new_model_new_dir = OUTPUT_PATH + timestamp\n",
    "os.mkdir(new_model_new_dir)\n",
    "curr_model_output_path = new_model_new_dir + \"/model_version_\" + timestamp + \".pth\"\n",
    "torch.save(model.state_dict(), curr_model_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save loss and accuracy from training to same directory as model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save loss and acc after training: \n",
    "#TODO: compare later with results from testing?\n",
    "\n",
    "len_loss = len(loss_list) # nrEpoch * nrIterationsPerEpoch\n",
    "#print(len_loss)\n",
    "\n",
    "#create pandas df with loss and acc from training:\n",
    "loss_acc_stats = pd.DataFrame(loss_list, columns=[\"train_loss\"])\n",
    "loss_acc_stats[\"train_acc\"] = acc_list\n",
    "new_model_new_dir\n",
    "\n",
    "path = new_model_new_dir + \"/loss_acc_stats.json\"\n",
    "loss_acc_stats.to_json(path)\n",
    "\n",
    "#loss_acc_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load former loss and accuracy in again (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load loss_acc_stats in again if needed:\n",
    "loaded_loss_acc_stats = pd.read_json(path)\n",
    "loaded_loss_acc_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of training loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "ax[0].plot(np.array(loaded_loss_acc_stats[\"train_loss\"]))\n",
    "ax[1].plot(np.array(loaded_loss_acc_stats[\"train_acc\"]))\n",
    "ax[0].set_xlabel(\"Steps\")\n",
    "ax[0].set_ylabel(\"Train Loss\")\n",
    "ax[1].set_xlabel(\"Steps\")\n",
    "ax[1].set_ylabel(\"Train Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First load in the model from directory\n",
    "\n",
    "When the training is done we can just upload our saved model from the directory we saved it to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO ist alles wichtige in dem model drin? \n",
    "#see: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "#test with current model:\n",
    "model_to_test = ThreeLayerCNN()\n",
    "model_to_test.load_state_dict(torch.load(curr_model_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO was ist mit unseren validierungsdaten? noch nicht genutzt bisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy on the test image patches\n",
    "\n",
    "Setting the model to evaluation mode by calling model.eval() disables drop-out and batch normalization layers in our model.\n",
    "The function call torch.no_grad() on the other hand disables the autograd functionality in our model. It is not needed for evaluation. Further it speeds up the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the model to evaluation mode\n",
    "model_to_test.eval()\n",
    "\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    tn = 0 \n",
    "    fn = 0 \n",
    "    fp = 0 \n",
    "    tp = 0 \n",
    "    total = 0\n",
    "    #for images, labels in test_dataloader:\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "        \n",
    "        #outputs = model(images)\n",
    "        outputs = model_to_test(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        #iterate over predicted and labels to get tp, fp, tn, tf:\n",
    "        for pred, lab in zip(predicted, labels):\n",
    "            if(pred == 0 and lab == 0):\n",
    "                tn += 1\n",
    "            if(pred == 0 and lab == 1):\n",
    "                fn += 1\n",
    "            if(pred == 1 and lab == 0):\n",
    "                fp += 1\n",
    "            if(pred ==1 and lab ==1):\n",
    "                tp += 1\n",
    "        \n",
    "            #befill confusion_matrix:\n",
    "            confusion_matrix[lab.long(), pred.long()] += 1\n",
    "        \n",
    "    \n",
    "    print('Anzahl Testdaten gesamt: ' + str(total))\n",
    "    print('Richtig vorhergesagt (TN und TP): ' + str(correct))\n",
    "    print('Falsch vorhergesagt (FN und FP): ' + str(total - correct))\n",
    "    print('True negative: ' + str(tn))\n",
    "    print('False negative: ' + str(fn))\n",
    "    print('False positive: ' + str(fp))\n",
    "    print('True positive: ' + str(tp))\n",
    "    print('Test Accuracy of the model on the 43313 test image patches: {} %'.format((correct / total) * 100))\n",
    "\n",
    "# Save the model checkpoint\n",
    "#curr_model_eval_path = new_model_new_dir + \"/model_eval_\" + timestamp + \".ckpt\"\n",
    "#torch.save(model_to_test.state_dict(), curr_model_eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#versuch umwandlung in numpy array, dann in pandas dataframe fÃ¼r einen plot (bisher nicht erfolgreich)\n",
    "matrix_as_arr = np.array(confusion_matrix)\n",
    "\n",
    "matrix_as_df = pd.DataFrame({'Predicted: 0': matrix_as_arr[:,0], 'Predicted: 1': matrix_as_arr[:,1]})\n",
    "matrix_as_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO vielleicht richtig visualisieren oder too much? mir egal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate precision and recall for our test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision\n",
    "precision = tp / (tp + fp) #precision denominator: total predicted positive\n",
    "\n",
    "#recall\n",
    "recall = tp / (tp + fn) #recall denominator: total actual positive\n",
    "\n",
    "print(\"Precision:\")\n",
    "print()\n",
    "print(\"Precision tells us how many of the overall predicted positive are actually true positive ones. Precision is a good measure to tell us if the cost of false positive is high.\")\n",
    "print(\"For our case: From the \" + str(tp + fp) + \" image patches that we predicted as having IDC, only \" +\n",
    "      str(tp) + \" patches actually contain IDC.\")\n",
    "print(\"Hence, only \" + str(precision * 100) + \"% are being predicted right.\")\n",
    "print(\"Depending on which patches belong to which image from which patient, we would have told quite some patients that they have IDC, even though they don't have it.\")\n",
    "print()\n",
    "print()\n",
    "print(\"Recall:\")\n",
    "print()\n",
    "print(\"Recall calculates how many of the actual positives from our model we captured through labeling them as positive. This metric tells us whether there is a high cost associated with false negative.\")\n",
    "print(\"For our case: From all the \" + str(tp + fn) + \" image patches that actually show IDC, we only predicted \" +\n",
    "     str(tp) + \" image patches correctly.\")\n",
    "print(\"Hence, \" + str(recall * 100) + \"% are being predicted right.\")\n",
    "print(\"Again depending on which patches belong to which image from which patient we would have told some patients they don't have IDC even though they have breast cancer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate F1 measure for our test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc f1\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "all_patches = tn + fp + fn + tp\n",
    "\n",
    "print(\"F1 score is needed when seeking a balance between precision and recall. \" +\n",
    "      \"Even though accuracy is a similar measurement, the accuracy can be influenced by a large number of true negatives \" +\n",
    "      \" (saying an uneven class distribution).\")\n",
    "print()\n",
    "print(\"In our case considering the test dataset, from the overall \" + str(all_patches) + \" patches, \" +\n",
    "      str(tn + fp) + \" patches contain no IDC, while \" + str(fn + tp) + \" contain IDC.\")\n",
    "print(\"So, \" + str(((tn + fp) / all_patches) * 100) + \"% of the patches contain no IDC and \" \n",
    "      + str(((fn + tp) / all_patches) * 100) + \"% have IDC in them.\")\n",
    "print(\"As shown above the number of true negatives (\" + str(tn) + \") is quite high in our test data.\")\n",
    "print()\n",
    "print(\"The F1 score for our test data is: \" + str(f1))\n",
    "#TODO noch mehr erlÃ¤utern???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "## TODO: Hier wird gerade nur ein einfacher plot von accuracy und loss gemacht, noch mehr machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO vervollstÃ¤ndigen; mehr visualisierung etc.\n",
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='Model results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save bokeh plot needs installation:\n",
    "#pip install selenium\n",
    "#this didn't work:\n",
    "#npm install -g phantomjs-prebuilt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export plot and save to directory:\n",
    "#from bokeh.io import export_png\n",
    "#file_path = new_model_new_dir + \"bokeh_plot.png\"\n",
    "#export_png(p, filename=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BreastCancer",
   "language": "python",
   "name": "breastcancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
